
# Parallel Histogram with MPI

A parallel histogramming program that uses **OpenMPI** on an HPC cluster with **Slurm**. This README walks you through:

1. [Folder Structure](#folder-structure)  
2. [Prerequisites](#prerequisites)  
3. [Compiling the Code](#compiling-the-code)  
4. [Running with Slurm](#running-with-slurm)  
5. [Experiment Setup & Parts (a), (b), (c)](#experiment-setup)  
6. [Additional Experiments](#additional-experiments)  
7. [Common Issues & Troubleshooting](#common-issues)

---

## 1. Folder Structure

A suggested organization:

```
MPI/
├─ parallel_histogram.cpp      # Your C++ MPI program
├─ run_histogram.sh            # Slurm job script to launch the program
├─ README.md                   # This file
└─ (any other scripts or data)
```

---

## 2. Prerequisites

1. **HPC Environment with Slurm**: The cluster allocates nodes via `sbatch` scripts.  
2. **MPI Compiler**: Typically `OpenMPI`, `MPICH`, or Intel MPI.  
3. **Module System**: On many clusters, you must load modules like `gcc` and `openmpi` before compiling.

---

## 3. Compiling the Code

1. **Log in** to your HPC cluster (e.g., `ssh yourusername@explorer...`).  
2. **Load required modules** (example):
   ```bash
   module purge
   module load OpenMPI/4.1.6
   ```
3. **Compile**:
   ```bash
  mpicxx -o parallel_histogram parallel_histogram.cpp
   ```
4. Verify the executable:
   ```bash
   ls -l parallel_histogram
   # should see a file named 'parallel_histogram' with executable permissions
   ```

---

## 4. Running with Slurm

Create a Slurm job script (e.g., `run_histogram.sh`):

```bash
#!/bin/bash

#SBATCH --verbose
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=16
#SBATCH --cpus-per-task=1
#SBATCH --time=00:30:00
#SBATCH --job-name=HistogramJob
#SBATCH --mem=100G
#SBATCH --partition=courses

module purge
module load OpenMPI/4.1.6

echo "Starting job on $SLURM_NNODES nodes, $SLURM_NTASKS_PER_NODE tasks per node"
echo "Total tasks: $SLURM_NTASKS"

# Example: Using mpirun
mpirun -np $SLURM_NTASKS ~/MPI/parallel_histogram 128

echo "Job completed."
```

1. **Edit** `--nodes`, `--ntasks-per-node`, and the final argument (`128`) as needed.  
2. **Submit** the job:
   ```bash
   sbatch run_histogram.sh
   ```
3. **Check** the resulting output file (e.g. `slurm-<jobid>.out` or `slurm-<jobid>.err`) to see the execution time, histogram bins, and any errors.

---

## 5. Experiment Setup & Parts (a), (b), (c)

### Part (a): 128 Bins

1. **Goal**: Run on 2, 4, and 8 nodes, measure performance.  
2. **Modify** `--nodes` in `run_histogram.sh` and pass **128** as the bin argument:
   ```bash
   mpirun -np $SLURM_NTASKS ~/MPI/parallel_histogram 128
   ```
3. **Submit** for each node count (2, 4, 8).  
4. **Record** the time from the output. Comment on speedup and scaling.

### Part (b): 32 Bins

1. **Goal**: Run on 2 and 4 nodes, measure performance.  
2. **Same** approach, but pass **32** bins:
   ```bash
   mpirun -np $SLURM_NTASKS ~/MPI/parallel_histogram 32
   ```
3. **Compare** the times with 128 bins and comment on differences.

### Part (c): Compare & Explain

1. **Compare** the 128‐bin runs (part a) vs. 32‐bin runs (part b) and discuss overhead.  
2. **Highlight**:
   - Communication overhead differences (fewer bins → smaller reduce messages).  
   - Potential HPC noise or random generation overhead.  
   - Why at certain node counts 32 bins might show bigger improvements, while at other node counts the difference is minimal.

---

## 6. Additional Experiments

- **Repeat Runs**: To account for variability, run each config multiple times and average.  
- **Vary Total Data**: Instead of 8 million, try 2M or 16M to see how scaling changes.  
- **Time Breakdown**: Insert additional `MPI_Wtime()` calls to see how much time is spent in generation vs. histogram updates vs. MPI_Reduce.  
- **Try Other Bin Counts**: 64, 256, or 512 bins to see how overhead scales.

---

## 7. Common Issues

1. **“No such file or directory”**: Make sure you compiled the code and that the path to `parallel_histogram` is correct.  
2. **PMI/PMIx Errors**: If you see “OPAL ERROR: Unreachable” or “... cannot execute with Slurm’s PMI,” try:
   - Using `mpirun` instead of `srun`, **or**  
   - Loading a module of OpenMPI compiled with Slurm support.  
3. **“mpirun: command not found”**: You must `module load openmpi/...` before `mpirun`.  
4. **Timing Variation**: HPC clusters often have noise. Repeat runs or measure averages if results fluctuate.

---

# Conclusion

This README provides the **end‐to‐end** process for setting up, compiling, and running a parallel histogram using **MPI** on an HPC cluster with Slurm. The key steps are:

1. **Load modules** (compiler + MPI).  
2. **Compile** using `mpicxx`.  
3. **Submit** a Slurm script that launches the program with `mpirun` or `srun`.  
4. **Analyze** the histogram results and runtime from the Slurm output.  
5. **Compare** different bin counts, node counts, and expansions to see how your application scales.

Feel free to customize the script commands, node counts, and bin counts to reflect your specific environment and assignment requirements!

# Sources

https://github.com/KhaledAshrafH/Ages-Distribution-Histogram/blob/main/main.c
https://github.com/stevenalbert/parallel-histogram/blob/master/openmpi/main.c
https://github.com/stevenalbert/parallel-histogram/blob/master/openmp/main.cpp
